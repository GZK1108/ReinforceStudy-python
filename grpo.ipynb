{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed初始化分布式环境\n",
    "if args.local_rank != -1:\n",
    "    deepspeed.int_distributed(dist_backend='nccl')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    logger.info(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个GRPOTrainer类来加载分词器和基础模型，加载lora权重\n",
    "self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)\n",
    "model_kwargs = {\n",
    "    'torch.dtype': torch.float16 if cuda_avilable else torch.float32,\n",
    "    'use_cache': False\n",
    "}\n",
    "self.model = AutoModelForCausalLM.from_pretrained(\n",
    "    self.base_model_path,\n",
    "    **model_kwargs\n",
    ")\n",
    "self.model = PeftModel.from_pretrained(\n",
    "    self.model,\n",
    "    self.lora_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora加载的参数默认不可训练，需要手动设置可训练\n",
    "trainable_params = []\n",
    "for name, param in self.model.name_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "        trainable_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f515eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed分布式封装，用于分布式训练\n",
    "ds_args = {\n",
    "    \"model\":self.model,\n",
    "    \"model_parameters\": trainable_params,\n",
    "    \"config\": ds_config\n",
    "}\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(**ds_args)\n",
    "self.model = model_engine\n",
    "self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集处理\n",
    "train_data = []\n",
    "if \"data\" in json_data:\n",
    "    logger.info(f\"find {len(json_data[\"data\"])} samples\")\n",
    "    for item in json_data[\"data\"]:\n",
    "        source_code = item.get(\"source_code\", \"\")\n",
    "        test_code = item.get(\"test_code\", \"\")\n",
    "        if not source_code:\n",
    "            continue\n",
    "        prompt = f\"请为以下Java类生成单元测试用例: ```java {source_code} ```生成的测试用例：\"\n",
    "        train_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"source_code\": source_code,\n",
    "            \"test_code\": test_code\n",
    "        })\n",
    "else:\n",
    "    logger.info(\"no data\")\n",
    "    train_data = json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写测试环境\n",
    "class TestGenerationEnvironment:\n",
    "    def __init__(self, jacoco_path:str = None, pit_path: str = None):\n",
    "        self.coverage_weight = 0.4\n",
    "        self.mutation_weight = 0.3\n",
    "        self.readability_weight = 0.3\n",
    "\n",
    "        # 创建临时工作目录\n",
    "        self.temp_dir = tempfile.mkdtemp(prefix=\"test_eval_\")\n",
    "        logger.info(f\"new temp work folder\")\n",
    "    \n",
    "    def evaluate_test(self,\n",
    "                      generated_test:str,\n",
    "                      source_code:str,\n",
    "                      reference_test:str = None) -> Tuple[float, Dict]:\n",
    "        logger.info(\"start test\")\n",
    "        try:\n",
    "            # 静态覆盖率分析\n",
    "            converage_score = self._static_coverage_analysis(generated_test)\n",
    "            # 静态变异分析\n",
    "            mutation_score = self._static_mutation_analysis(generated_test)\n",
    "            # 计算可读性分数\n",
    "            readability_score = self._calculate_readability(generated_test)\n",
    "            # 参考测试\n",
    "            similarity_score = 0.0\n",
    "            if reference_test:\n",
    "                similarity_score = self._calculate_similarity(generated_test, reference_test)\n",
    "            \n",
    "            # 总分\n",
    "            total_score = self.coverage_weight * coverage_score + self.mutation_weight * mutation_score + self.readability_weight * readability_score\n",
    "            logger.info(f\"total score: {total_score}\")\n",
    "\n",
    "            return total_score, {\n",
    "                \"coverage_score\": coverage_score,\n",
    "                \"mutation_score\": mutation_score,\n",
    "                \"readability_score\": readability_score,\n",
    "                \"similarity_score\": similarity_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"测试评估过程中出现未捕获的错误: {str(e)}\")\n",
    "            return 0.65, {                \n",
    "                \"coverage_score\": 0.6,\n",
    "                \"mutation_score\": 0.6,\n",
    "                \"readability_score\": 0.8,\n",
    "                \"similarity_score\": 0.0,\n",
    "                \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试样本\n",
    "try:\n",
    "    generated_test = self.generated_test(prompt, num_samples=num_samples)\n",
    "except Exception as e:\n",
    "    return {\"loss\": 0, \"mean_reward\": 0, \"mean_kl_div\": 0, \"num_samples\": 0}\n",
    "\n",
    "try:\n",
    "    rewards, metrics_list = self.compute_rewards(generated_test, source_code, reference_test)\n",
    "except Excetion as e:\n",
    "    return {\"loss\": 0, \"mean_reward\": 0, \"mean_kl_div\": 0, \"num_samples\": 0}\n",
    "\n",
    "value = rewards.mean()\n",
    "\n",
    "advantages = rewards - value\n",
    "\n",
    "if len(advantages) > 1 and advantages.std() >0:\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprobs(self, model, inputs, attention_mask=None):\n",
    "    model_inputs = {\"input_ids\": input_ids}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
